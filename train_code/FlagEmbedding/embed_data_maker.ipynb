{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdxuser/miniconda3/envs/torch24/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import json\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"Qwen/Qwen2.5-14B-Instruct-AWQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_df = pd.read_csv('../comp_data/train.csv')\n",
    "mis_map = pd.read_csv('../comp_data/misconception_mapping.csv')\n",
    "\n",
    "mis_map['MisconceptionName'] = mis_map['MisconceptionName'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape: (1496, 15), eval_df.shape: (373, 15)\n"
     ]
    }
   ],
   "source": [
    "eval_df = full_train_df.iloc[:373]\n",
    "train_df = full_train_df.iloc[373:]\n",
    "\n",
    "print(f\"train_df.shape: {train_df.shape}, eval_df.shape: {eval_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold_data = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(train_df)):\n",
    "    fold_data.append({\n",
    "        'train': train_df.iloc[train_idx],\n",
    "        'val': train_df.iloc[val_idx],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1196it [00:00, 7059.22it/s]\n",
      "300it [00:00, 7014.12it/s]\n",
      "1197it [00:00, 7170.06it/s]\n",
      "299it [00:00, 7186.47it/s]\n",
      "1197it [00:00, 7088.10it/s]\n",
      "299it [00:00, 7250.50it/s]\n",
      "1197it [00:00, 7147.82it/s]\n",
      "299it [00:00, 7086.29it/s]\n",
      "1197it [00:00, 4645.58it/s]\n",
      "299it [00:00, 7258.64it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_query_template():\n",
    "    return \"\"\"Subject: {SubjectName}\n",
    "\n",
    "Construct: {ConstructName}\n",
    "\n",
    "Question:\n",
    "{QuestionText}\n",
    "\n",
    "Correct Option:\n",
    "{CorrectAnswerText}\n",
    "\n",
    "Incorrect Option 1:\n",
    "{IncorrectAnswer1Text}\n",
    "\n",
    "Incorrect Option 2:\n",
    "{IncorrectAnswer2Text}\n",
    "\n",
    "Incorrect Option 3:\n",
    "{IncorrectAnswer3Text}\"\"\"\n",
    "\n",
    "def get_task_description():\n",
    "    return 'Given a math question with options, retrieve the most relevant misconceptions for the incorrect answers.'\n",
    "\n",
    "def process_data(df, mis_map):\n",
    "    train_data = []\n",
    "    for idx, row in tqdm(df.iterrows()):\n",
    "        correct_option = row['CorrectAnswer']\n",
    "        correct_option_text = row[f'Answer{correct_option}Text']\n",
    "        \n",
    "        incorrect_options_texts = [row[f'Answer{option}Text'] for option in ['A', 'B', 'C', 'D'] if option != correct_option]\n",
    "\n",
    "        query_text = get_query_template().format(**{\n",
    "            'SubjectName': row['SubjectName'],\n",
    "            'ConstructName': row['ConstructName'],\n",
    "            'QuestionText': row['QuestionText'],\n",
    "            'CorrectAnswerText': correct_option_text,\n",
    "            'IncorrectAnswer1Text': incorrect_options_texts[0],\n",
    "            'IncorrectAnswer2Text': incorrect_options_texts[1],\n",
    "            'IncorrectAnswer3Text': incorrect_options_texts[2],\n",
    "        })\n",
    "        \n",
    "        related_mis_ids = [row[f'Misconception{option}Id'] for option in ['A', 'B', 'C', 'D']]\n",
    "        related_mis_ids = [mis_id for mis_id in related_mis_ids if pd.notna(mis_id)]\n",
    "        related_mis_ids = list(set(related_mis_ids))\n",
    "        related_mis_texts = [mis_map.loc[mis_id]['MisconceptionName'] for mis_id in related_mis_ids]\n",
    "        \n",
    "        train_data.append({\n",
    "            \"query\": query_text,\n",
    "            \"pos\": related_mis_texts,\n",
    "            \"neg\": [],\n",
    "            \"prompt\": get_task_description(),\n",
    "        })\n",
    "    return train_data\n",
    "\n",
    "# Process each fold\n",
    "fold_train_data = []\n",
    "for fold_dict in fold_data:\n",
    "    fold_train_data.append({\n",
    "        'train': process_data(fold_dict['train'], mis_map),\n",
    "        'val': process_data(fold_dict['val'], mis_map)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1496it [00:00, 7063.03it/s]\n"
     ]
    }
   ],
   "source": [
    "all_fold_train_data = process_data(train_df, mis_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = 'stage1_data'\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "for fold, fold_dict in enumerate(fold_train_data):\n",
    "    with open(f'{SAVE_DIR}/fold_{fold}_train.jsonl', 'w') as f:\n",
    "        for item in fold_dict['train']:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "    with open(f'{SAVE_DIR}/fold_{fold}_val.jsonl', 'w') as f:\n",
    "        for item in fold_dict['val']:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "            \n",
    "with open(f'{SAVE_DIR}/train_all_folds.jsonl', 'w') as f:\n",
    "    for item in all_fold_train_data:\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_pool = []\n",
    "for idx, row in mis_map.iterrows():\n",
    "    candidate_pool.append({\n",
    "        \"text\": row['MisconceptionName'],\n",
    "    })\n",
    "    \n",
    "with open(f'{SAVE_DIR}/candidate_pool.jsonl', 'w') as f:\n",
    "    for data in candidate_pool:\n",
    "        f.write(json.dumps(data) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:00<00:00,  7.57it/s]\n",
      "inferencing embedding for corpus (number=2587)--------------\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Inference Embeddings: 100%|███████████████████████| 3/3 [00:07<00:00,  2.39s/it]\n",
      "inferencing embedding for queries (number=1196)--------------\n",
      "Inference Embeddings: 100%|█████████████████████| 13/13 [00:23<00:00,  1.81s/it]\n",
      "create index and search------------------\n",
      "Batches: 100%|██████████████████████████████████| 19/19 [00:00<00:00, 20.04it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:00<00:00,  9.67it/s]\n",
      "inferencing embedding for corpus (number=2587)--------------\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Inference Embeddings: 100%|███████████████████████| 3/3 [00:07<00:00,  2.40s/it]\n",
      "inferencing embedding for queries (number=1197)--------------\n",
      "Inference Embeddings: 100%|█████████████████████| 13/13 [00:23<00:00,  1.80s/it]\n",
      "create index and search------------------\n",
      "Batches: 100%|██████████████████████████████████| 19/19 [00:00<00:00, 22.15it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:00<00:00, 10.09it/s]\n",
      "inferencing embedding for corpus (number=2587)--------------\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Inference Embeddings: 100%|███████████████████████| 3/3 [00:07<00:00,  2.43s/it]\n",
      "inferencing embedding for queries (number=1197)--------------\n",
      "Inference Embeddings: 100%|█████████████████████| 13/13 [00:23<00:00,  1.82s/it]\n",
      "create index and search------------------\n",
      "Batches: 100%|██████████████████████████████████| 19/19 [00:00<00:00, 23.59it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:00<00:00,  9.34it/s]\n",
      "inferencing embedding for corpus (number=2587)--------------\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Inference Embeddings: 100%|███████████████████████| 3/3 [00:07<00:00,  2.44s/it]\n",
      "inferencing embedding for queries (number=1197)--------------\n",
      "Inference Embeddings: 100%|█████████████████████| 13/13 [00:23<00:00,  1.82s/it]\n",
      "create index and search------------------\n",
      "Batches: 100%|██████████████████████████████████| 19/19 [00:00<00:00, 23.58it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:00<00:00,  9.31it/s]\n",
      "inferencing embedding for corpus (number=2587)--------------\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Inference Embeddings: 100%|███████████████████████| 3/3 [00:07<00:00,  2.39s/it]\n",
      "inferencing embedding for queries (number=1197)--------------\n",
      "Inference Embeddings: 100%|█████████████████████| 13/13 [00:23<00:00,  1.81s/it]\n",
      "create index and search------------------\n",
      "Batches: 100%|██████████████████████████████████| 19/19 [00:00<00:00, 23.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# bash\n",
    "for i_fold in range(5):\n",
    "    ! CUDA_VISIBLE_DEVICES=0 python scripts/hn_mine.py \\\n",
    "    --embedder_name_or_path BAAI/bge-en-icl \\\n",
    "    --input_file stage1_data/fold_{i_fold}_train.jsonl \\\n",
    "    --output_file stage1_data/fold_{i_fold}_train_minedHN.jsonl \\\n",
    "    --candidate_pool stage1_data/candidate_pool.jsonl \\\n",
    "    --range_for_sampling 2-150 \\\n",
    "    --negative_number 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
